{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T20:33:35.799179Z",
     "start_time": "2020-11-12T20:33:35.788926Z"
    }
   },
   "source": [
    "# [Sentiment Analysis](https://www.kaggle.com/c/sentiment-analysis-pmr3508)\n",
    "\n",
    "[PMR3508](https://uspdigital.usp.br/jupiterweb/obterDisciplina?sgldis=PMR3508) - Machine Learning and Pattern Recognition\n",
    "\n",
    "Professor Fabio Gagliardi Cozman\n",
    "\n",
    "PMR3508-2020-83 - [Vitor Gratiere Torres](https://github.com/vitorgt/PMR3508)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment's goal is to tell whether an IMDb review is positive or negative (complementary).\n",
    "\n",
    "My analysis takes the following steps:\n",
    "\n",
    "1. [Import data and python modules](#1.-Import-data-and-python-modules) (Acquisition)\n",
    "1. [Preprocessing](#2.-Preprocessing)\n",
    "    1. [Clean text and vectorize](#2.1.-Clean-text-and-vectorize) (Preprocessing)\n",
    "    1. [Embed words](#2.2.-Embed-words) (Representation)\n",
    "1. [Modelling](#3.-Modelling)\n",
    "    1. [Models Definition and Hyperparameters Search](#3.1.-Models-Definition-and-Hyperparameters-Search)\n",
    "        1. [Logistic Regression](#3.1.1.-Logistic-Regression)\n",
    "        1. [K-nearest Neighbors](#3.1.2.-K-nearest-Neighbors)\n",
    "        1. [Multi-layer Perceptron - 1 Hidden Layer](#3.1.3.-Multi-layer-Perceptron---1-Hidden-Layer)\n",
    "        1. [Multi-layer Perceptron - 2 Hidden Layers](#3.1.4.-Multi-layer-Perceptron---2-Hidden-Layers)\n",
    "        1. [PyTorch Multi-layer Perceptron - 2 Hidden Layers](#3.1.5.-PyTorch-Multi-layer-Perceptron---2-Hidden-Layers)\n",
    "    1. [Metric Comparison](#3.2.-Metric-Comparison)\n",
    "1. [Submission](#4.-Submission)\n",
    "\n",
    "# 1. Import data and python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:39:50.653160Z",
     "start_time": "2020-12-06T00:39:50.647344Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import loguniform\n",
    "from skopt.space import Integer, Real\n",
    "\n",
    "# Text modules\n",
    "import re\n",
    "import string\n",
    "from ftfy import fix_text\n",
    "\n",
    "# Embeddings\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyperparameters\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:40:05.579044Z",
     "start_time": "2020-12-06T00:40:05.563113Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "XYtrain = pd.read_csv(\"sentiment-analysis-pmr3508/data_train.csv\")\n",
    "# kaggle path = \"../input/sentiment-analysis-pmr3508/data_XYtrain.csv\"\n",
    "XYtest = pd.read_csv(\"sentiment-analysis-pmr3508/data_test1.csv\")\n",
    "Xsubmit = pd.read_csv(\"sentiment-analysis-pmr3508/data_test2_X.csv\")\n",
    "\n",
    "print(\"Training data shape:\", XYtrain.shape)\n",
    "print(XYtrain.describe())\n",
    "\n",
    "print(\"\\nTest data shape:\", XYtest.shape)\n",
    "print(XYtest.describe())\n",
    "\n",
    "print(\"\\nSubmission data shape:\", Xsubmit.shape)\n",
    "print(Xsubmit.describe())\n",
    "\n",
    "XYtrain.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "\n",
    "The nature of this dataset is different from previous, there might not be ```NAs```, ```NANs``` or ```Nulls```, this time we ought to notice duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:40:09.331587Z",
     "start_time": "2020-12-06T00:40:09.310917Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Training data shape initially:\", XYtrain.shape)\n",
    "XYtrain = XYtrain.drop_duplicates(keep=\"first\")\n",
    "print(\"Training data shape without duplicates:\", XYtrain.shape)\n",
    "\n",
    "print(\"\\nTest data shape initially:\", XYtest.shape)\n",
    "XYtest = XYtest.drop_duplicates(keep=\"first\")\n",
    "print(\"Test data shape without duplicates:\", XYtest.shape)\n",
    "\n",
    "print(\"\\nSubmission data shape initially:\", Xsubmit.shape)\n",
    "Xsubmit = Xsubmit.drop_duplicates(keep=\"first\")\n",
    "print(\"Submission data shape without duplicates:\", Xsubmit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:40:11.846573Z",
     "start_time": "2020-12-06T00:40:11.828182Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain = XYtrain.loc[:, \"review\"]\n",
    "Ytrain = XYtrain.loc[:, \"positive\"]\n",
    "\n",
    "Xtest = XYtest.loc[:, \"review\"]\n",
    "Ytest = XYtest.loc[:, \"positive\"]\n",
    "\n",
    "Xsubmit = Xsubmit.iloc[:, 0]\n",
    "\n",
    "del XYtrain\n",
    "del XYtest\n",
    "\n",
    "print(\"Xtrain\", Xtrain.shape, \"Ytrain\", Ytrain.shape)\n",
    "print(\"Xtest\", Xtest.shape, \"Ytest\", Ytest.shape)\n",
    "print(\"Xsubmit\", Xsubmit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are all ```pandas.Series``` now.\n",
    "\n",
    "## 2.1. Clean text and vectorize\n",
    "\n",
    "We were given the below function to clean and prepare the dataset. This is an important step dealing with NLP, we could make use of NLTK or spaCy, but we rather a manual approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:40:15.405103Z",
     "start_time": "2020-12-06T00:40:15.369119Z"
    }
   },
   "outputs": [],
   "source": [
    "def preptext(txt):\n",
    "    # removing tags\n",
    "    txt = txt.replace(\"<br />\", \" \")\n",
    "    # fixing Mojibakes (See https://pypi.org/project/ftfy/)\n",
    "    txt = fix_text(txt)\n",
    "    # converting case\n",
    "    txt = txt.lower()\n",
    "    # removing punctuation\n",
    "    txt = txt.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # removing hyphens\n",
    "    txt = txt.replace(\" â€” \", \" \")\n",
    "    # replacing digits with a tag\n",
    "    txt = re.sub(\"\\d+\", \" <number> \", txt)\n",
    "    # removing double spaces\n",
    "    txt = re.sub(\" +\", \" \", txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def clean_split(data):\n",
    "    # let's apply this function to clean the dataset\n",
    "    data = data.apply(preptext)\n",
    "\n",
    "    # now I need a vector of words, I'll split those strings\n",
    "    data = data.apply(lambda x: x.split())\n",
    "\n",
    "    print(data.sample(3))\n",
    "    return data\n",
    "\n",
    "\n",
    "Xtrain = clean_split(Xtrain)\n",
    "Xtest = clean_split(Xtest)\n",
    "Xsubmit = clean_split(Xsubmit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Embed words\n",
    "\n",
    "It is a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension $f:\\; \\text{word} \\hookrightarrow \\mathbb{R}^{50}$.\n",
    "\n",
    "We'll use a pre-trained Doc2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:40:37.725056Z",
     "start_time": "2020-12-06T00:40:37.716525Z"
    }
   },
   "outputs": [],
   "source": [
    "def embed(txt, model):\n",
    "    # model.random.seed(42)\n",
    "    x = model.infer_vector(txt, steps=20)\n",
    "    return x\n",
    "\n",
    "\n",
    "d2v = Doc2Vec.load(\"sentiment-analysis-pmr3508/doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:40:40.724215Z",
     "start_time": "2020-12-06T00:40:40.689064Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Embedding Xtrain\")\n",
    "srt = time.perf_counter()\n",
    "Xtrain = Xtrain.apply(embed, model=d2v)\n",
    "Xtrain = pd.DataFrame(Xtrain.to_list())\n",
    "end = time.perf_counter()\n",
    "print(\"Xtrain shape:\", Xtrain.shape)\n",
    "print(f\"Embedding Xtrain done in {end - srt:.1f}s\\n\")\n",
    "\n",
    "print(\"Embedding Xtest\")\n",
    "srt = time.perf_counter()\n",
    "Xtest = Xtest.apply(embed, model=d2v)\n",
    "Xtest = pd.DataFrame(Xtest.to_list())\n",
    "end = time.perf_counter()\n",
    "print(\"Xtest shape:\", Xtest.shape)\n",
    "print(f\"Embedding Xtest done in {end - srt:.1f}s\\n\")\n",
    "\n",
    "print(\"Embedding Xsubmit\")\n",
    "srt = time.perf_counter()\n",
    "Xsubmit = Xsubmit.apply(embed, model=d2v)\n",
    "Xsubmit = pd.DataFrame(Xsubmit.to_list())\n",
    "end = time.perf_counter()\n",
    "print(\"Xsubmit shape:\", Xsubmit.shape)\n",
    "print(f\"Embedding Xsubmit done in {end - srt:.1f}s\\n\")\n",
    "\n",
    "Xtrain.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is ready!\n",
    "\n",
    "# 3. Modelling\n",
    "\n",
    "## 3.1. Models Definition and Hyperparameters Search\n",
    "\n",
    "Now, using ```sklearn.model_selection.RandomizedSearchCV```, we'll search for the best hyperparameters for each classifier maximizing Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "### 3.1.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:40:55.243865Z",
     "start_time": "2020-12-06T00:40:55.226016Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Model definition\n",
    "logreg = LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "# Hyperparameters\n",
    "logreg_hp = dict(\n",
    "    # Inverse of regularization strength; must be a positive float.\n",
    "    # Like in support vector machines, smaller values specify\n",
    "    # stronger regularization.\n",
    "    C=np.linspace(0, 10, 100),\n",
    "    # Used to specify the norm used in the penalization.\n",
    "    penalty=[\"l1\", \"l2\"],\n",
    ")\n",
    "\n",
    "# Research\n",
    "logreg_researcher = RandomizedSearchCV(\n",
    "    logreg,\n",
    "    logreg_hp,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=2,\n",
    "    n_iter=50,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "logreg_results = logreg_researcher.fit(Xtrain, Ytrain)\n",
    "\n",
    "# Result\n",
    "print(logreg_results.best_params_, logreg_results.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. K-nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:41:00.870447Z",
     "start_time": "2020-12-06T00:41:00.852676Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Model definition\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Hyperparameters\n",
    "knn_hp = dict(\n",
    "    # Number of neighbors to use.\n",
    "    n_neighbors=np.arange(151, 210, 1),\n",
    ")\n",
    "\n",
    "# Research\n",
    "knn_researcher = RandomizedSearchCV(\n",
    "    knn,\n",
    "    knn_hp,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=2,\n",
    "    n_iter=50,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "knn_results = knn_researcher.fit(Xtrain, Ytrain)\n",
    "\n",
    "# Result\n",
    "print(knn_results.best_params_, knn_results.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Multi-layer Perceptron - 1 Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:41:06.289075Z",
     "start_time": "2020-12-06T00:41:06.258802Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Model definition\n",
    "mlp_1h = MLPClassifier(early_stopping=True)\n",
    "\n",
    "# Hyperparameters\n",
    "mlp_1h_hp = dict(\n",
    "    # The ith element represents the number of neurons in the ith\n",
    "    # hidden layer.\n",
    "    hidden_layer_sizes=[(2 ** i,) for i in np.arange(6, 12)],\n",
    "    # L2 penalty (regularization term) parameter.\n",
    "    alpha=loguniform(0.000001, 0.1),\n",
    ")\n",
    "\n",
    "# Research\n",
    "mlp_1h_researcher = RandomizedSearchCV(\n",
    "    mlp_1h,\n",
    "    mlp_1h_hp,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=2,\n",
    "    n_iter=30,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "mlp_1h_results = mlp_1h_researcher.fit(Xtrain, Ytrain)\n",
    "\n",
    "# Result\n",
    "print(mlp_1h_results.best_params_, mlp_1h_results.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4. Multi-layer Perceptron - 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:41:11.873431Z",
     "start_time": "2020-12-06T00:41:11.848652Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Model definition\n",
    "mlp_2h = MLPClassifier(early_stopping=True)\n",
    "\n",
    "# Hyperparameters\n",
    "mlp_2h_hp = dict(\n",
    "    # The ith element represents the number of neurons in the ith\n",
    "    # hidden layer.\n",
    "    hidden_layer_sizes=[\n",
    "        (2 ** i, 2 ** j)\n",
    "        for j in np.arange(6, 10)\n",
    "        for i in np.arange(6, 10)\n",
    "    ],\n",
    "    # L2 penalty (regularization term) parameter.\n",
    "    alpha=loguniform(0.000001, 0.1),\n",
    ")\n",
    "\n",
    "# Research\n",
    "mlp_2h_researcher = RandomizedSearchCV(\n",
    "    mlp_2h,\n",
    "    mlp_2h_hp,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=2,\n",
    "    n_iter=30,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "mlp_2h_results = mlp_2h_researcher.fit(Xtrain, Ytrain)\n",
    "\n",
    "# Result\n",
    "print(mlp_2h_results.best_params_, mlp_2h_results.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5. PyTorch Multi-layer Perceptron - 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:41:21.616772Z",
     "start_time": "2020-12-06T00:41:21.583817Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Model definition\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, hidden1_dim=512, hidden2_dim=64, p=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        # 50 -> hidden1_dim -> hidden2_dim -> 2\n",
    "        self.fc1 = nn.Linear(50, hidden1_dim)\n",
    "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.fc3 = nn.Linear(hidden2_dim, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        fc_out = F.relu(self.fc1(X))\n",
    "        fc_out = self.dropout(fc_out)\n",
    "\n",
    "        fc_out = F.relu(self.fc2(fc_out))\n",
    "        fc_out = self.dropout(fc_out)\n",
    "\n",
    "        fc_out = self.fc3(fc_out)\n",
    "        soft_out = F.softmax(fc_out, dim=-1)\n",
    "\n",
    "        return soft_out\n",
    "\n",
    "\n",
    "# Model definition\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_2h = NeuralNetClassifier(\n",
    "    MLPNet().to(device),\n",
    "    max_epochs=20,\n",
    "    lr=1e-4,\n",
    "    optimizer=optim.Adam,\n",
    "    optimizer__weight_decay=1e-4,\n",
    "    train_split=False,\n",
    "    verbose=0,\n",
    "    iterator_train__shuffle=True,\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "torch_2h_hp = dict(\n",
    "    module__hidden1_dim=Integer(256, 2048),\n",
    "    module__hidden2_dim=Integer(64, 1024),\n",
    "    module__p=Real(0.1, 0.75, prior=\"uniform\"),\n",
    "    optimizer__weight_decay=Real(1e-10, 1e-2, prior=\"log-uniform\"),\n",
    ")\n",
    "\n",
    "# Research\n",
    "torch_2h_researcher = BayesSearchCV(\n",
    "    torch_2h,\n",
    "    torch_2h_hp,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=2,\n",
    "    n_iter=30,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "torch_2h_results = torch_2h_researcher.fit(\n",
    "    Xtrain.to_numpy(np.float32), Ytrain.to_numpy(np.int64)\n",
    ")\n",
    "\n",
    "# Result\n",
    "print(torch_2h_results.best_params_, torch_2h_results.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Metric Comparison\n",
    "\n",
    "Based on ```Test``` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:42:09.396550Z",
     "start_time": "2020-12-06T00:42:09.372630Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_test_score = roc_auc_score(\n",
    "    Ytest, logreg_results.predict_proba(Xtest)[:, 1]\n",
    ")\n",
    "print(f\"Logistic Regression: {logreg_test_score:.5f}\")\n",
    "\n",
    "knn_test_score = roc_auc_score(\n",
    "    Ytest, knn_results.predict_proba(Xtest)[:, 1]\n",
    ")\n",
    "print(f\"K-nearest Neighbors: {knn_test_score:.5f}\")\n",
    "\n",
    "mlp_1h_test_score = roc_auc_score(\n",
    "    Ytest, mlp_1h_results.predict_proba(Xtest)[:, 1]\n",
    ")\n",
    "print(f\"Multi-layer Perceptron 1 Hidden: {mlp_1h_test_score:.5f}\")\n",
    "\n",
    "mlp_2h_test_score = roc_auc_score(\n",
    "    Ytest, mlp_2h_results.predict_proba(Xtest)[:, 1]\n",
    ")\n",
    "print(f\"Multi-layer Perceptron 2 Hidden: {mlp_2h_test_score:.5f}\")\n",
    "\n",
    "torch_2h_test_score = roc_auc_score(\n",
    "    Ytest, torch_2h_results.predict_proba(Xtest)[:, 1]\n",
    ")\n",
    "print(f\"PyTorch Multi-layer Perceptron 2 Hidden: {torch_2h_test_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T00:52:16.513606Z",
     "start_time": "2020-12-06T00:52:16.500027Z"
    }
   },
   "outputs": [],
   "source": [
    "Ysubmit = torch_2h_results.predict_proba(Xsubmit)[:, 1]\n",
    "Ysubmit = pd.DataFrame({\"positive\": Ysubmit})\n",
    "Ysubmit.to_csv(\"submission.csv\", index=True, index_label=\"Id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
